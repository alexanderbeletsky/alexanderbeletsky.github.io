<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: nodejs | Alexander Beletsky's development blog]]></title>
  <link href="http://beletsky.net/blog/categories/nodejs/atom.xml" rel="self"/>
  <link href="http://beletsky.net/"/>
  <updated>2014-07-28T17:54:19+03:00</updated>
  <id>http://beletsky.net/</id>
  <author>
    <name><![CDATA[Alexander Beletsky]]></name>
    <email><![CDATA[alexander.beletsky@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Secure Deployment of ElasticSearch]]></title>
    <link href="http://beletsky.net/2014/07/secure-deployment-of-elasticsearch.html"/>
    <updated>2014-07-18T12:34:00+03:00</updated>
    <id>http://beletsky.net/2014/07/secure-deployment-of-elasticsearch</id>
    <content type="html"><![CDATA[<p>Remember my success <a href="http://beletsky.net/2014/05/got-tired-of-mongodb-full-text.html">story</a> of moving from MongoDB full text search index to <a href="http://www.elasticsearch.com">ElasticSearch</a>? After about one month of work our search service unexpectedly stopped. I received a bunch of emails from <a href="http://logentiries.com">Logentries</a> and <a href="http://newrelic.com">NewRelic</a> that server is no longer responsive.</p>

<p>Once I logged on to <a href="https://www.digitalocean.com/?refcode=de56d081b272">DigitalOcean</a> account, I&rsquo;ve seen message from administrators that server is sending a lot of traffic (UDP Flood), very likely compromised and therefore stopped. The link they give as <a href="https://www.digitalocean.com/community/questions/my-droplet-is-locked-by-support-staff-because-because-of-an-outgoing-flood-or-ddos-what-do-i-do">instructions</a> to fix the problem was quite comprehensive, but the most important info was in comments. A lot of people who were hit by problem had ElasticSearch deployed on their machines.</p>

<!-- MORE -->


<p>It appeared, if ES is left on default configuration with port opened outside, it will be easy target for bad guys, both of Java and ES vulnerability. Basically, I had to restore server from scratch, but this time I&rsquo;m not goint to be naive, server have to be properly secured.</p>

<p>I&rsquo;ll describe my setup that involves: Node.js, Dokku / Docker, SSL.</p>

<h2>Clean DigitalOcean image</h2>

<p>I had to reinstall my machine from scratch, so instead of creating plain Ubuntu 14 server, but I decided to go dokku/docker path. Something that I tried <a href="http://beletsky.net/2013/09/paas-in-your-pocket-with-dokku.html">before</a> and very happy with the <a href="http://beletsky.net/2013/08/digitalocean-plus-dokku-equals-10-heroku.html">results</a>. <a href="https://www.digitalocean.com/?refcode=de56d081b272">DigitalOcean</a> offers pre-packed image with dokku/docker already on board. As usually it takes just a few seconds to spin up machine on DO.</p>

<p>The plan was the following: deploy ElasticSearch instance inside Docker container, disable dynamic script features of Elastic, deploy Node.js based proxy server with custom authentication by Dokku and link those containers, so only Node.js proxy will have access to Elastic. Finally, all traffic between will by crypted by SSL.</p>

<p>The benefits are obvious, no more <code>:9200</code> is outside the machine, one potential vulnerability with dynamic script feature is disabled, only authenticated client could access Elastic server.</p>

<h2>Deployment of ElasticSearch in Docker</h2>

<p>First we need to have Docker image of ElasticSearch. There few ES plugging for Dokku, I decided to install one by myself, since I thought it is easier to configure. There is a great instruction from Docker <a href="https://registry.hub.docker.com/u/dockerfile/elasticsearch/">here</a>.</p>

<p><code>bash
$ docker pull docker pull dockerfile/elasticsearch
</code></p>

<p>Once the image is there, we need to prepare volume there Elastic stores data and configuration.</p>

<p><code>bash
$ cd /
$ mkdir elastic
</code></p>

<p>Elastic folder should contain <code>elasticsearch.yml</code> file, with all required configurations. My case is very simple, I have a cluster of one machine, so default configuration applies to me. One thing, that I mentioned above &ndash; I need to disable dynamic scripting features.</p>

<p><code>bash
$ nano elasticsearch.yml
</code></p>

<p>The content of the file is just one string,</p>

<p><code>plain
script.disable_dynamic: true
</code></p>

<p>Once it&rsquo;s done, we are ready to launch the server inside Docker container. Since, it could be done few times (during configuration and debugging), I&rsquo;ve created a small shell script,</p>

<p><code>bash
docker run --name elastic -d -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -v /elastic:/data dockerfile/elasticsearch /elasticsearch/bin/elasticsearch -Des.config=/data/elasticsearch.yml
</code></p>

<p>Please note the important thing, <code>-p 127.0.0.1:9200:9200</code> &ndash; here we are binding <code>:9200</code> to be only accessible on <code>localhost</code>. I&rsquo;ve spend some hours to close <code>:9200</code> by <code>iptables</code> without any success, but that thing works as expected. Thanks a lot to <a href="https://twitter.com/darkproger">@darkproger</a> and <a href="https://twitter.com/kkdoo">@kkdoo</a> for great help.</p>

<p><code>-v /elastic:/data</code> will map the containers volume <code>/data</code> to local one <code>/elastic</code>.</p>

<h2>Front-end Node.js server</h2>

<p>Now, we need to deploy front-end proxy server. It would proxy all traffic from <code>http://localhost:9200</code> into outside world, securely. I&rsquo;ve created small project, based on <a href="https://github.com/nodejitsu/node-http-proxy">http-proxy</a> called <a href="https://github.com/likeastore/elastic-proxy">elastic-proxy</a>. It&rsquo;s very simple one and can be easily re-used.</p>

<p><code>bash
$ git clone https://github.com/likeastore/elastic-proxy
$ cd elastic-proxy
</code></p>

<p>The server itself,</p>

<p>```js
var http = require(&lsquo;http&rsquo;);
var httpProxy = require(&lsquo;http-proxy&rsquo;);
var url = require(&lsquo;url&rsquo;);</p>

<p>var config = require(&lsquo;./config&rsquo;);
var logger = require(&lsquo;./source/utils/logger&rsquo;);</p>

<p>var port = process.env.PORT || 3010;
var proxy = httpProxy.createProxyServer();</p>

<p>var parseAccessToken = function (req) {</p>

<pre><code>var request = url.parse(req.url, true).query;
var referer = url.parse(req.headers.referer || '', true).query;

return request.access_token || referer.access_token;
</code></pre>

<p>};</p>

<p>var server = http.createServer(function (req, res) {</p>

<pre><code>var accessToken = parseAccessToken(req);

logger.info('request: ' + req.url + ' accessToken: ' + accessToken + ' referer: ' + req.headers.referer);

if (!accessToken || accessToken !== config.accessToken) {
    res.statusCode = 401;
    return res.end('Missing access_token query parameter');
}

proxy.web(req, res, {target: config.target, rejectUnauthorized: false});
</code></pre>

<p>});</p>

<p>server.listen(port, function () {</p>

<pre><code>logger.info('Likeastore Elastic-Proxy started at: ' + port);
</code></pre>

<p>});
```</p>

<p>It proxies all the requests and only by-pass ones, who provide <code>access_token</code> as query parameter. The <code>access_token</code> value is <a href="https://github.com/likeastore/elastic-proxy/blob/master/config/production.config.js">configured</a> on server, by applications environment variable <code>PROXY_ACCESS_TOKEN</code>.</p>

<p>As you already <a href="https://www.digitalocean.com/community/tutorials/how-to-use-the-dokku-one-click-digitalocean-image-to-deploy-a-python-flask-app">prepared</a> Dokku all you need to do is to push application to your server.</p>

<p><code>bash
$ git push master production
</code></p>

<p>After the deployment, you should go to server to configure applications environment,</p>

<p><code>bash
$ dokku config proxy set PROXY_ACCESS_TOKEN="your_secret_value"
</code></p>

<p>I wanted to have SSL and it&rsquo;s very easy to configure with Dokku. Just place your <code>server.crt</code> and <code>server.key</code> to <code>/home/dokku/proxy/tls</code> folder.</p>

<p>Proxy have to be <a href="https://github.com/scottatron/dokku-rebuild">redeployed</a> afterward, to apply configuration changes. Try proxy by hitting it&rsquo;s URL, in my case <a href="https://search.likeastore.com">https://search.likeastore.com</a>. If everything is good, it will reply,</p>

<p><code>plain
Missing access_token query parameter
</code></p>

<h2>Link containers</h2>

<p>We need to link both containers, so Node.js application is able to access ElasticSeach container. I really liked <a href="https://github.com/rlaneve/dokku-link">dokku-link</a> plugin, that does exactly that&rsquo;s required in very easy way. So, will install it</p>

<p><code>bash
$ cd /var/lib/dokku/plugins
$ git clone https://github.com/rlaneve/dokku-link
</code></p>

<p>Now, we need to link containers,</p>

<p><code>bash
$ dokku link proxy elastic
</code></p>

<p>And application have to be redeployed again. If everything is good, you will be able to access the server by <code>https://proxy.yourserver.com?access_token=your_secret_value</code> and see the response from ElasticSearch,</p>

<p>```js
{</p>

<pre><code>status: 200,
name: "Tundra",
version: {
    number: "1.2.1",
    build_hash: "6c95b759f9e7ef0f8e17f77d850da43ce8a4b364",
    build_timestamp: "2014-06-03T15:02:52Z",
    build_snapshot: false,
    lucene_version: "4.8"
},
tagline: "You Know, for Search"
</code></pre>

<p>}
```</p>

<h2>Reconfigure the client</h2>

<p>Depending on a client you use, you need to apply a little tweak to configuration. Basically, we need to use <code>access_token</code> for all our request. For Node.js applications,</p>

<p>```js
var client = elasticsearch.Client({</p>

<pre><code>host: {
    protocol: 'https',
    host: 'search.likeastore.com',
    port: 443,
    query: {
        access_token: process.env.ELASTIC_ACCESS_TOKEN
    }
},
requestTimeout: 5000
</code></pre>

<p>});
```</p>

<p>Again, <code>ELASTIC_ACCESS_TOKEN</code> is env variable to hold <code>your_secret_value</code> token.</p>

<p>Now, restart the application, make sure everything is running and exhale.</p>

<p><strong>PS</strong>. I would like to say thank to <a href="https://www.digitalocean.com/?refcode=de56d081b272">DigitalOcean</a> for support and a little credit I received as downtime compensation. Thats awesome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Integration or Unit Tests Trade-Off]]></title>
    <link href="http://beletsky.net/2014/04/integration-tests-for-express-dot-js-middleware.html"/>
    <updated>2014-04-23T11:19:00+03:00</updated>
    <id>http://beletsky.net/2014/04/integration-tests-for-express-dot-js-middleware</id>
    <content type="html"><![CDATA[<p>Recently, I&rsquo;ve released small Express.js extension for easy switching application to <code>maintenance</code> mode. Sometimes, you just want to run patch against database or change the infrastructure of product, but instead of showing blank <code>nginx</code> 503 error page, you want to have nice looking HTML, saying will be back soon.</p>

<p>The <code>maintenance</code> package is now available on <a href="https://www.npmjs.org/package/maintenance">npm</a> and you welcome to use it. But I would like to share the way I developed and test it.</p>

<!-- More -->


<p>So, <code>maintenance</code> is a function that takes <code>app</code> instance and <code>options</code> as a parameters. Then it augments <code>app</code> with additional endpoint (if there is such preference) and injects middleware function that would render maintenance page in case of mode is set to <code>true</code>.</p>

<p>It&rsquo;s really small piece functionality, but still I wanted to make sure, it&rsquo;s gonna work in my application without annoying restarts of servers and debugging the stuff. And TDD is right approach to solve the pain.</p>

<p>TDD is quite commonly trade-off of <code>unit tests</code> and <code>integration tests</code>. And it&rsquo;s always up to developer which direction go in particular case. Let&rsquo;s take a look on my case:</p>

<p>If you go <code>unit test</code> way, it would be something like that:</p>

<ul>
<li>call <code>maintenance</code> function, pass mock of <code>express</code> instance

<ul>
<li>verify that new <code>route</code> is added after function completes</li>
<li>verify that all <code>route.callbacks</code> now have callback to check the mode</li>
<li>verify that <code>route.callbacks[0]</code> contains mode check function</li>
</ul>
</li>
<li>call <code>route.callbacks[0]</code>, mock <code>req</code> and <code>res</code> object

<ul>
<li>verify that <code>res.render</code> called with <code>maintanance.html</code> argument</li>
<li>etc..</li>
</ul>
</li>
</ul>


<p>Is all of that kind of <strong>suck</strong>? It is, since it would take too much effort for mocking the stuff.. But more important, all of that is nothing more as just testing of <strong>implementation details</strong> of Express.js, but not <strong>behavior details</strong> of application.</p>

<p>Now, let&rsquo;s consider <code>integration tests</code> way for same stuff:</p>

<ul>
<li>run application in normal mode

<ul>
<li>send HTTP GET and make sure that response is fine</li>
</ul>
</li>
<li>run application is maintenance mode

<ul>
<li>send HTTP GET and make sure that response contains maintenance page</li>
</ul>
</li>
<li>run application in normal mode

<ul>
<li>send HTTP GET and make sure that response is fine</li>
<li>sent HTTP POST to maintenance endpoint</li>
<li>send HTTP GET and make sure that response contains maintenance page</li>
</ul>
</li>
</ul>


<p>Does it look like real <strong>behavior</strong> testing? Indeed, test acts as <code>user</code> and checks that application actually behaves right, doesn&rsquo;t matter of implementation details.</p>

<p>With spending more time with Express.js and Node.js I see much <a href="http://beletsky.net/2014/03/testable-apis-with-node-dot-js.html">more value</a> in integration way of testing. It&rsquo;s very easy to spin-up a server and send HTTP requests and check responses.</p>

<p>If you are interested, check out <a href="https://github.com/likeastore/maintenance/blob/master/test/spec/maintenence.spec.js">specification</a> and <a href="https://github.com/likeastore/maintenance/blob/master/test/app/app.js">test application</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testable API's with Node.js]]></title>
    <link href="http://beletsky.net/2014/03/testable-apis-with-node-dot-js.html"/>
    <updated>2014-03-04T15:36:00+02:00</updated>
    <id>http://beletsky.net/2014/03/testable-apis-with-node-dot-js</id>
    <content type="html"><![CDATA[<p>API is heart of modern web application. It&rsquo;s all about to make it easy to consume, scale and make sure it works as expected. Currently I follow &ldquo;all open API methods must have tests&rdquo; (AOAMMHT) principle. I used to work with .NET technologies, where testing of API&rsquo;s was about calling methods of corresponding controller object, which typically was unit testing &ndash; mocking up all controller dependencies, setting up expects of returned values.</p>

<p>I&rsquo;ve changed my mind on testing with Node.js/Express.js development. For API&rsquo;s I prefer &ldquo;end-to-end&rdquo; testing: setting up user account, authentication, HTTP calls to server, real calls to DB and serving JSON payload back. API&rsquo;s have to be tested from consumer point of few to be able to give some meaningful results.</p>

<!-- MORE -->


<h2>Tools and Frameworks</h2>

<p>Pretty standard setup: <a href="http://visionmedia.github.io/mocha/">mocha</a>, <a href="http://chaijs.com/">chai</a>, <a href="https://github.com/mikeal/request">request</a>.</p>

<p>Mocha is time proven tool for testing Node.js applications, Chai is good enough expectation framework and Request as one best HTTP clients I even worked with.</p>

<h2>Prepare application for testing</h2>

<p>Dependencies above is installed via <code>npm install</code> and should be saved to <code>package.json</code> file (with <code>--save</code> option). Your project structure should have <code>test</code> folder inside, which mocha is using as default to look tests inside. Few files should be added there, current structure I have that works.</p>

<p>```plain
/test</p>

<pre><code>/specs
    auth.spec.js
    ...
common.js
mocha.opts
utils.js
runMocha.js
</code></pre>

<p>```</p>

<p><code>/api</code> folder is the one that would contain specifications for you API, <code>mocha.opts</code> &ndash; contains global mocha configuration, <code>common.js</code> is common require file, that all tests are using, <code>utils.js</code> &ndash; test helper that would contain everything you need during testing, <code>runMocha.js</code> utility that would be tests entry point.</p>

<h3>mocha.opts</h3>

<p><code>plain
--require ./test/common.js
--reporter spec
--ui bdd
--recursive
--colors
--timeout 60000
--slow 300
</code></p>

<p>Mocha options allow to require some additional javascript file, as well as setting up global Mocha settings as what reporter to use, timeouts etc.</p>

<h3>common.js</h3>

<p><code>js
global.chai = require('chai');
global.expect = global.chai.expect;
</code></p>

<p>To do not require <code>chai</code> in each spec files, it&rsquo;s possible to require it once and place to global scope.</p>

<h3>runMocha.js</h3>

<p>```js
process.env.NODE_ENV = process.env.NODE_ENV || &lsquo;test&rsquo;;
process.env.TEST_ENV = process.env.TEST_ENV || &lsquo;test&rsquo;;</p>

<p>var exit = process.exit;
process.exit = function (code) {</p>

<pre><code>setTimeout(function () {
    exit();
}, 200);
</code></pre>

<p>};</p>

<p>require(&lsquo;../source/server&rsquo;);
require(&lsquo;../node_modules/mocha/bin/_mocha&rsquo;);
```</p>

<p>The secret sauce is last 2 lines. Since <code>require</code> is synchronous we first &ldquo;call&rdquo; API server to get up and after that &ldquo;call&rdquo; mocha engine to start testing. So, inside each tests we can do real HTTP calls to real HTTP servers. No mocks.</p>

<h3>package.json</h3>

<p>```js
// &hellip;
&ldquo;scripts&rdquo;: {</p>

<pre><code>"test": "node test/runMocha",
</code></pre>

<p>  },
// &hellip;
```</p>

<p>Package file should contain script to call API tests with simple <code>npm test</code> command.</p>

<h2>Test driven API</h2>

<p>Mocha is using BDD (behaviour driven development) approach to testing. Comparing to classical TDD, BDD encourage to write specifications in plain English, which works good especially when you just starting with particular feature.</p>

<p>What I typically do is just writing down specification of API, without any thinking of how to implement it. Give you example of something that I worked with recently.</p>

<p>```js
var request = require(&lsquo;request&rsquo;);
var testUtils = require(&lsquo;../utils&rsquo;);</p>

<p>describe(&lsquo;collections.spec.js&rsquo;, function () {</p>

<pre><code>describe('when non authorized', function () {
    it ('should not be authorized', function () {
    });
});

describe('when authorized', function () {
    describe('when new collection created', function () {
        describe('public', function () {
            it('should respond with 201 (created)', function () {
            });

            it('should create new collection', function () {
            });

            it('should have user', function () {
            });

            it('should collection be public', function () {
            });

            describe('and title is missing', function () {
                it('should respond with 412 (bad request)', function () {
                });
            });

            describe('with description', function () {
                it('should respond with 201 (created)', function () {
                });

                it('should create new collection', function () {
                });
            });
        });
    });

    // etc..
});
</code></pre>

<p>});
```</p>

<p>This is kind of skeleton I to have before start anything else.</p>

<h3>Unauthorized access</h3>

<p>If your API or part of it requires authorization, I prefer to test it.</p>

<p>```js</p>

<pre><code>var token, user, url, headers, response, results;

beforeEach(function () {
    url = testUtils.getRootUrl() + '/api/collections';
});

describe('when non authorized', function () {
    beforeEach(function (done) {
        request.get({url: url, json: true}, function (err, resp, body) {
            response = resp;
            done(err);
        });
    });

    it ('should not be authorized', function () {
        expect(response.statusCode).to.equal(401);
    });
});
</code></pre>

<p>```</p>

<p><code>testUtils.getRootUrl()</code> returns qualified URL for API, depending on test environment. During development, it&rsquo;s just <code>http://localhost:3000</code> where your <code>server.js</code> started.</p>

<h3>Authorized access</h3>

<p>Authorized access typically requires some kind of <code>access_token</code> sent either by headers or query string. Doesn&rsquo;t matter how, but <code>utils.js</code> must have method that would create new user and obtain access token from API. The actual implementation of such method would depend on your API auth mechanism.</p>

<p>All tests that required authorization access, should have such <code>beforeEach()</code>,</p>

<p>```js
beforeEach(function (done) {</p>

<pre><code>testUtils.createTestUserAndLoginToApi(function (err, createdUser, accessToken) {
    token = accessToken;
    user = createdUser;
    headers = {'X-Access-Token': accessToken};
    done(err);
});
</code></pre>

<p>});
```</p>

<p>After <code>access_token</code> is acquired, it can be used as part of any authorized calls.</p>

<h3>Behavior tests</h3>

<p>Now, everything is ready to test the behavior of API. Nothing fancy here, just act as clients do. Send HTTP requests, receive responses and check HTTP statuses. I&rsquo;ll just post some code, so it would give you direction.</p>

<p>```js
describe(&lsquo;when new collection created&rsquo;, function () {</p>

<pre><code>describe('public', function () {
    beforeEach(function () {
        collection = {title: 'This is test collection', public: true};
    });

    beforeEach(function (done) {
        request.post({url: url, headers: headers, body: collection, json: true}, function (err, resp, body) {
            response = resp;
            results = body;
            done(err);
        });
    });

    it('should respond with 201 (created)', function () {
        expect(response.statusCode).to.equal(201);
    });

    it('should create new collection', function () {
        expect(results.title).to.be.ok;
        expect(results._id).to.be.ok;
    });

    it('should have user', function () {
        expect(results.user).to.equal(user.email);
    });

    it('should collection be public', function () {
        expect(results.public).to.equal(true);
    });

    describe('and title is missing', function () {
        beforeEach(function (done) {
            request.post({url: url, headers: headers, body: {}, json: true}, function (err, resp, body) {
                response = resp;
                results = body;
                done(err);
            });
        });

        it('should respond with 412 (bad request)', function () {
            expect(response.statusCode).to.equal(412);
        });
    });

    describe('with description', function () {
        beforeEach(function () {
            collection = {title: 'This is test collection', description: 'description'};
        });

        beforeEach(function (done) {
            request.post({url: url, headers: headers, body: collection, json: true}, function (err, resp, body) {
                response = resp;
                results = body;
                done(err);
            });
        });

        it('should respond with 201 (created)', function () {
            expect(response.statusCode).to.equal(201);
        });

        it('should create new collection', function () {
            expect(results.description).to.equal('description');
        });
    });
});
</code></pre>

<p>});
```</p>

<h2>Conclusions</h2>

<p>I think dynamic languages as JavaScript is great for testing API&rsquo;s. Having no types eliminates &ldquo;model-per-response&rdquo; classes, <code>request.js</code> is great for making HTTP calls and <code>mocha</code> makes specifications output looks nice. So, nevertheless of back-end technology you can try to use the approach and see how it works for you.</p>

<p>The setup of tests and starting up of API services are so lightweight in Node.js that makes test-first API development nice and pleasant thing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Keeping Node.js Processes Running]]></title>
    <link href="http://beletsky.net/2014/01/keeping-node-dot-js-processes-running.html"/>
    <updated>2014-01-18T13:38:00+02:00</updated>
    <id>http://beletsky.net/2014/01/keeping-node-dot-js-processes-running</id>
    <content type="html"><![CDATA[<p>Node.js/Express.js is great for Web API&rsquo;s and applications. In contrast to known enterprise technologies, Node.js is very special. It&rsquo;s single process/threaded environment. In case of unhanded exception occurred Node.js virtual machine simply stops, leaving application in unresponsive state.</p>

<p>Due to <code>async</code> nature of Node.js <code>try/catch</code> not always works, even with <code>domains</code> and stuff you have a chance that application crashed on production while you sleep.</p>

<!-- More -->


<p>To mitigate the issue few <a href="http://stackoverflow.com/questions/1972242/auto-reload-of-files-in-node-js">known solutions</a> exist, common idea is that there is watchdog that keeping eye on <code>node</code> process and if crashed, restarts application again.</p>

<p>Recently I&rsquo;ve used great library by <a href="https://github.com/mafintosh">@mafintosh</a> called <a href="https://github.com/mafintosh/respawn">respawn</a>. I liked it&rsquo;s minimalistic style and decided to try it out.</p>

<p>The bare-bones code is very simple. Without modification of your application, just create file <code>monitor.js</code> with nearly such code:</p>

<p>```js
var respawn = require(&lsquo;respawn&rsquo;);</p>

<p>var monitor = respawn([&lsquo;node&rsquo;, &lsquo;server.js&rsquo;], {</p>

<pre><code>env: {ENV_VAR:'test'}, // set env vars
cwd: '.',              // set cwd
maxRestarts:10,        // how many restarts are allowed within 60s
sleep:1000,            // time to sleep between restarts
</code></pre>

<p>});</p>

<p>monitor.start(); // spawn and watch
```</p>

<p><code>monitor</code> will spawn new node process and in case of crash it will be restarted. You can also specify <code>maxRestars</code> (I recommend to do that, if something is really bad it won&rsquo;t be restarted infinitely) and <code>sleep</code> time.</p>

<p>I&rsquo;ve tried that, by implementing <code>/fail</code> end-point in my app, to see that <code>respawn</code> really works.</p>

<p>```js
app.get(&lsquo;/fail&rsquo;, function (req, res, next) {</p>

<pre><code>setTimeout(function () {
    var nu = null;
    nu.access();

    res.send('Hello World');
}, 1000);
</code></pre>

<p>});
```</p>

<p>if I try to hit <code>/fail</code> I&rsquo;ll see no results in browser, but if I go back to <code>/</code> the application is running in normal state.</p>

<p>But simple respawning of application is not complete solution. You need to know what exactly happened to be able to fix issue. <a href="http://beletsky.net/2013/07/think-ahead-think-logging.html">Proper logging</a> of your application is essential. I&rsquo;ll show my small setup around <code>respawn</code> that send critical message to <a href="https://logentries.com">Logentries</a>, so all crashes are logged.</p>

<p>```js
var respawn = require(&lsquo;respawn&rsquo;);
var util = require(&lsquo;util&rsquo;);
var logger = require(&lsquo;./source/utils/logger&rsquo;);</p>

<p>var proc = respawn([&lsquo;node&rsquo;, &lsquo;app.js&rsquo;], {</p>

<pre><code>cwd: '.',
maxRestarts: 10,
sleep: 1000,
</code></pre>

<p>});</p>

<p>proc.on(&lsquo;spawn&rsquo;, function () {</p>

<pre><code>util.print('application monitor started...');
</code></pre>

<p>});</p>

<p>proc.on(&lsquo;exit&rsquo;, function (code, signal) {</p>

<pre><code>logger.fatal({msg: 'process exited, code: ' + code + ' signal: ' + signal});
</code></pre>

<p>});</p>

<p>proc.on(&lsquo;stdout&rsquo;, function (data) {</p>

<pre><code>util.print(data.toString());
</code></pre>

<p>});</p>

<p>proc.on(&lsquo;stderr&rsquo;, function (data) {</p>

<pre><code>logger.error({msg: 'process error', data: data.toString()});
</code></pre>

<p>});</p>

<p>proc.start();
```</p>

<p>(details of logger you can find in this <a href="http://beletsky.net/2013/07/think-ahead-think-logging.html">post</a>)</p>

<p>All process output is goes to <code>stdout</code>, which is convinient for development, but in case of <code>stderr</code> or <code>exit</code> everything is logged to cloud and notification to <code>dev-team</code> sent.</p>

<p>It worked really nice, now I&rsquo;m not worry even if something bad happens on production, <code>respawn</code> will make sure that rest of users are not affected. As a developer you can much quicker found bug and push hotfix.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Several Processes in Docker Container]]></title>
    <link href="http://beletsky.net/2013/12/run-several-processes-in-docker-container.html"/>
    <updated>2013-12-27T14:15:00+02:00</updated>
    <id>http://beletsky.net/2013/12/run-several-processes-in-docker-container</id>
    <content type="html"><![CDATA[<p>What I like the most about <a href="">Docker</a> project is new opportunity to deploy and distribute software. Many times I&rsquo;ve been to situation when I wanted to play with some software and get exited about, but after I read installation manual my excitement totally gone. Non trivial applications, requires quite a lot dependencies: runtimes, libraries, databases.</p>

<p>With docker, the installation instruction got reduced to something like:</p>

<p><code>
$ docker pull vendor/package
$ docker run vendor/package
</code></p>

<p>Simply like that, forget about missing Java Runtime on your server. It suits perfectly for TCP/HTTP applications.</p>

<!-- More -->


<p>Being messing around <a href="https://github.com/seismolabs/seismo">Seismo</a> project I realized, I want to go exactly same way. Since it has few dependencies now, MongoDB and NodeJS &ndash; it should be easier to anyone to try it, even if they do not use that setup. I was happy to see, that GitHub currently offers great support for Docker. Namely, if you have repo with <code>Dockerfile</code> inside, each time you push the code, docker image got rebuild and pushed to public <a href="https://index.docker.io/">index</a>.</p>

<p>I&rsquo;ve created <code>Dockerfile</code> that would build up image, ready to have Seismo run inside.</p>

<p>```plain
FROM    ubuntu:latest</p>

<h1>Git</h1>

<p>RUN apt-get install -y git</p>

<h1>MongoDB</h1>

<p>RUN apt-key adv &mdash;keyserver hkp://keyserver.ubuntu.com:80 &mdash;recv 7F0CEB10
RUN echo &lsquo;deb <a href="http://downloads-distro.mongodb.org/repo/ubuntu-upstart">http://downloads-distro.mongodb.org/repo/ubuntu-upstart</a> dist 10gen&rsquo; | tee /etc/apt/sources.list.d/10gen.list
RUN dpkg-divert &mdash;local &mdash;rename &mdash;add /sbin/initctl
RUN ln -s /bin/true /sbin/initctl
RUN apt-get update
RUN apt-get install mongodb-10gen
RUN mkdir -p /data/db</p>

<h1>NodeJS</h1>

<p>RUN apt-get update &mdash;fix-missing &amp;&amp; apt-get upgrade -y
RUN apt-get install -y wget curl build-essential patch git-core openssl libssl-dev unzip ca-certificates
RUN curl <a href="http://nodejs.org/dist/v0.10.22/node-v0.10.22-linux-x64.tar.gz">http://nodejs.org/dist/v0.10.22/node-v0.10.22-linux-x64.tar.gz</a> | tar xzvf &ndash; &mdash;strip-components=1 -C &ldquo;/usr&rdquo;
RUN apt-get clean &amp;&amp; rm -rf /var/cache/apt/archives/<em> /var/lib/apt/lists/</em></p>

<h1>Seismo</h1>

<p>RUN git clone <a href="https://github.com/seismolabs/seismo.git">https://github.com/seismolabs/seismo.git</a> /seismo
RUN cd /seismo; npm install
ENV PORT 8080
EXPOSE 8080</p>

<p>WORKDIR /seismo
ENTRYPOINT [&ldquo;./bin/run.sh&rdquo;]
```</p>

<p>It&rsquo;s based on latest Ubuntu server, installs Git, MongoDB and NodeJS runtime and clones Seismo itself inside image.</p>

<p>But, I&rsquo;ve met a problem to start few processes inside the container. Since I need both MongoDB for storage and NodeJS for API server, it&rsquo;s required both be running inside one container. If shell script just starts one, <code>mongod</code> for example, <code>node app.js</code> is not executed.</p>

<p>I was a little worried, thinking it&rsquo;s not possible to run more that one process inside container.</p>

<p>But solution was found. I&rsquo;ve created another shell script that starts <code>mongod</code> as background process and starts <code>node</code> after.</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<p>mongod &amp; node ./source/server.js
```</p>

<p>That worked as charm.</p>
]]></content>
  </entry>
  
</feed>
